{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d883f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (7000, 18)\n",
      "test  shape: (3000, 17)\n",
      "           ID  나이  키(cm)  몸무게(kg)    BMI    시력  충치  공복 혈당  혈압  중성 지방  \\\n",
      "0  TRAIN_0000  35    170       70  24.22  1.10   1     98  40     80   \n",
      "1  TRAIN_0001  40    150       55  24.44  1.00   0    173  39    104   \n",
      "2  TRAIN_0002  60    170       50  17.30  0.75   0     96  40     61   \n",
      "\n",
      "   혈청 크레아티닌  콜레스테롤  고밀도지단백  저밀도지단백  헤모글로빈  요 단백  간 효소율  label  \n",
      "0       1.3    211      75     120   15.9     1   1.53      1  \n",
      "1       0.6    251      46     184   11.8     1   1.45      0  \n",
      "2       0.8    144      43      89   15.3     1   1.04      0  \n",
      "\n",
      "사용 피처: ['나이', '키(cm)', '몸무게(kg)', 'BMI', '시력', '충치', '공복 혈당', '혈압', '중성 지방', '혈청 크레아티닌', '콜레스테롤', '고밀도지단백', '저밀도지단백', '헤모글로빈', '요 단백', '간 효소율']\n",
      "X shape: (7000, 16) / y shape: (7000,)\n",
      "\n",
      "X_train: (5600, 16) / X_valid: (1400, 16)\n",
      "\n",
      "=== RandomForest 학습 ===\n",
      "\n",
      "=== XGBoost 학습 ===\n",
      "\n",
      "=== 단일 모델 Valid 성능 (thr = 0.5) ===\n",
      "RF  Valid Acc : 0.7471\n",
      "XGB Valid Acc : 0.7350\n",
      "\n",
      "=== Soft Voting + Threshold 튜닝 시작 ===\n",
      "\n",
      "=== Soft Voting + Threshold 최종 결과 ===\n",
      "Best RF weight : 0.750\n",
      "Best XGB weight: 0.250\n",
      "Best threshold : 0.470\n",
      "Best Valid Acc : 0.7586\n",
      "\n",
      "=== 전체 데이터로 최종 학습 ===\n",
      "\n",
      "✅ submission_KTE_6.csv 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. 라이브러리 & 시드 고정\n",
    "# ============================================\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. 데이터 로드\n",
    "# ============================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(\"train shape:\", train.shape)\n",
    "print(\"test  shape:\", test.shape)\n",
    "print(train.head(3))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. Feature / Label 분리 (원래 16개 피처만)\n",
    "# ============================================\n",
    "X = train.drop(columns=[\"ID\", \"label\"])\n",
    "y = train[\"label\"].values\n",
    "\n",
    "X_test_final = test.drop(columns=[\"ID\"])\n",
    "\n",
    "print(\"\\n사용 피처:\", X.columns.tolist())\n",
    "print(\"X shape:\", X.shape, \"/ y shape:\", y.shape)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Train / Valid 분리 (항상 쓰던 방식 그대로)\n",
    "# ============================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nX_train:\", X_train.shape, \"/ X_valid:\", X_valid.shape)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. 모델 정의 (검증된 셋업 중심)\n",
    "#    - 이 조합에서 0.7586 근처가 나왔음\n",
    "# ============================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=1100,\n",
    "    max_depth=None,          # 깊이 제한 X (지금 데이터에서는 이게 좋았음)\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.07,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Train / Valid 학습\n",
    "# ============================================\n",
    "print(\"\\n=== RandomForest 학습 ===\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== XGBoost 학습 ===\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "rf_valid_proba  = rf.predict_proba(X_valid)[:, 1]\n",
    "xgb_valid_proba = xgb.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# 개별 모델 성능 (참고)\n",
    "rf_pred_05  = (rf_valid_proba  >= 0.5).astype(int)\n",
    "xgb_pred_05 = (xgb_valid_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== 단일 모델 Valid 성능 (thr = 0.5) ===\")\n",
    "print(f\"RF  Valid Acc : {accuracy_score(y_valid, rf_pred_05):.4f}\")\n",
    "print(f\"XGB Valid Acc : {accuracy_score(y_valid, xgb_pred_05):.4f}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Soft Voting + Threshold 미세 튜닝\n",
    "#    - RF 가중치 0.70 ~ 0.85\n",
    "#    - threshold 0.44 ~ 0.50\n",
    "#    - 이전에 잘 나온 구간만 집중 탐색 (시간 절약 + 과탐색 방지)\n",
    "# ============================================\n",
    "best_acc   = 0.0\n",
    "best_thr   = 0.47\n",
    "best_w_rf  = 0.75\n",
    "best_w_xgb = 0.25\n",
    "\n",
    "print(\"\\n=== Soft Voting + Threshold 튜닝 시작 ===\")\n",
    "\n",
    "rf_weights = np.arange(0.70, 0.86, 0.01)   # 0.70 ~ 0.85\n",
    "thr_list   = np.arange(0.44, 0.501, 0.002) # 0.44 ~ 0.50\n",
    "\n",
    "for w_rf in rf_weights:\n",
    "    w_xgb = 1.0 - w_rf\n",
    "    ens_proba = rf_valid_proba * w_rf + xgb_valid_proba * w_xgb\n",
    "\n",
    "    for thr in thr_list:\n",
    "        pred = (ens_proba >= thr).astype(int)\n",
    "        acc  = accuracy_score(y_valid, pred)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc   = acc\n",
    "            best_thr   = thr\n",
    "            best_w_rf  = w_rf\n",
    "            best_w_xgb = w_xgb\n",
    "\n",
    "print(\"\\n=== Soft Voting + Threshold 최종 결과 ===\")\n",
    "print(f\"Best RF weight : {best_w_rf:.3f}\")\n",
    "print(f\"Best XGB weight: {best_w_xgb:.3f}\")\n",
    "print(f\"Best threshold : {best_thr:.3f}\")\n",
    "print(f\"Best Valid Acc : {best_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 8. 전체 데이터로 최종 학습\n",
    "# ============================================\n",
    "print(\"\\n=== 전체 데이터로 최종 학습 ===\")\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=1100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_final = XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.07,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_final.fit(X, y)\n",
    "xgb_final.fit(X, y)\n",
    "\n",
    "rf_test_proba  = rf_final.predict_proba(X_test_final)[:, 1]\n",
    "xgb_test_proba = xgb_final.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "final_proba = rf_test_proba * best_w_rf + xgb_test_proba * best_w_xgb\n",
    "test_pred   = (final_proba >= best_thr).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# 9. 제출 파일 생성\n",
    "# ============================================\n",
    "sample_submission[\"label\"] = test_pred\n",
    "sample_submission.to_csv(\"submission_KTE_6.csv\", index=False)\n",
    "print(\"\\n✅ submission_KTE_6.csv 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f86c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
