{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6af68b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "plt.rcParams['font.family'] = 'Apple SD Gothic Neo'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(42) # Seed 고정\n",
    "# pd.read_csv() 함수를 사용해서 데이터를 읽어오는 코드입니다.\n",
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# pd.read_csv() 함수를 사용해서 데이터를 읽어오는 코드입니다.\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbbd9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    log_loss,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "def evaluate_model(model, X_valid, y_valid, name=\"model\", verbose=True):\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    # Log Loss (로스 로스, log_loss)\n",
    "    loss = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            loss = log_loss(X_valid, y_proba)\n",
    "        except Exception:\n",
    "            loss = None\n",
    "\n",
    "    # R² (결정계수, R-squared)\n",
    "    r2 = r2_score(y_valid, y_pred)\n",
    "\n",
    "    # 정확도(Accuracy)\n",
    "    acc = None\n",
    "    cls_report = None\n",
    "    try:\n",
    "        acc = accuracy_score(y_valid, y_pred)\n",
    "        cls_report = classification_report(y_valid, y_pred)\n",
    "    except Exception:\n",
    "        # 회귀모델(continuous target)일 때는 여기로 옴\n",
    "        pass\n",
    "\n",
    "    # MSE / RMSE\n",
    "    mse = mean_squared_error(y_valid, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # ------------ 출력 (verbose=True일 때만) ------------\n",
    "    if verbose:\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"[{name}] 모델 성능 평가\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        if loss is not None:\n",
    "            print(\"Log Loss:\", loss)\n",
    "\n",
    "        print(f\"{name:15s} R score(): {r2:.4f}\")\n",
    "\n",
    "        if acc is not None:\n",
    "            print(\"정확도:\", acc)\n",
    "            if cls_report is not None:\n",
    "                print(cls_report)\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"MSE (평균 제곱 오차): {mse:.3f}\")\n",
    "        print(f\"RMSE (평균 제곱근 오차): {rmse:.3f}\")\n",
    "        print(f\"R² Score (결정계수): {r2:.3f}\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "    # 핵심: 정확도(없으면 None)를 리턴\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0fb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(columns={'혈압': '맥압'}, inplace=True)\n",
    "test.rename(columns={'혈압': '맥압'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0b6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['시력'] > 2, '시력'] = 2.0\n",
    "train.loc[train['혈청 크레아티닌'] > 1.5, '혈청 크레아티닌'] = 1.5\n",
    "train.loc[train['요 단백'] > 4, '요 단백'] = 4.0\n",
    "train.loc[train['저밀도지단백'] > 150, '저밀도지단백'] = 150\n",
    "train.loc[train['고밀도지단백'] > 100, '고밀도지단백'] = 100\n",
    "train.loc[train['중성 지방'] > 200, '중성 지방'] = 200\n",
    "train.loc[train['공복 혈당'] > 200, '공복 혈당'] = 200\n",
    "\n",
    "train.loc[train['저밀도지단백'] < 22, '저밀도지단백'] = 22\n",
    "train.loc[train['고밀도지단백'] < 22, '고밀도지단백'] = 22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f4303a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['시력'] > 2, '시력'] = 2.0\n",
    "test.loc[test['혈청 크레아티닌'] > 1.5, '혈청 크레아티닌'] = 1.5\n",
    "test.loc[test['요 단백'] > 4, '요 단백'] = 4.0\n",
    "test.loc[test['저밀도지단백'] > 150, '저밀도지단백'] = 150\n",
    "test.loc[test['고밀도지단백'] > 100, '고밀도지단백'] = 100\n",
    "test.loc[test['중성 지방'] > 200, '중성 지방'] = 200\n",
    "test.loc[test['공복 혈당'] > 200, '공복 혈당'] = 200\n",
    "test.loc[test['간 효소율'] > 10, '간 효소율'] = 10\n",
    "\n",
    "test.loc[test['저밀도지단백'] < 22, '저밀도지단백'] = 22\n",
    "test.loc[test['고밀도지단백'] < 22, '고밀도지단백'] = 22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f220605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['저밀도지단백_고밀도지단백_낮음'] = train['저밀도지단백'] / train['고밀도지단백']\n",
    "train['고밀도지단백_저밀도지단백_높음'] = train['고밀도지단백'] / train['저밀도지단백']\n",
    "train['중성지방_고밀도지단백_2이하'] = train['중성 지방'] / train['고밀도지단백']\n",
    "train['true_BMI'] = train['몸무게(kg)'] / ((train['키(cm)'] / 100) ** 2)\n",
    "train['총 콜레스테롤'] = train['저밀도지단백'] +train['고밀도지단백'] + (train['중성 지방'] / 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aaaf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['저밀도지단백_고밀도지단백_낮음'] = test['저밀도지단백'] / test['고밀도지단백']\n",
    "test['고밀도지단백_저밀도지단백_높음'] = test['고밀도지단백'] / test['저밀도지단백']\n",
    "test['중성지방_고밀도지단백_2이하'] = test['중성 지방'] / test['고밀도지단백']\n",
    "test['true_BMI'] = test['몸무게(kg)'] / ((test['키(cm)'] / 100) ** 2)\n",
    "test['총 콜레스테롤'] = test['저밀도지단백'] +test['고밀도지단백'] + (test['중성 지방'] / 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b84baafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['BMI','콜레스테롤'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d0eaa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=['BMI','콜레스테롤'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "940362e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['label','ID',], axis = 1)\n",
    "y = train['label']\n",
    "X_test = test.drop(['ID',], axis = 1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6945a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d7f3",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dd6b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 모델 (Linear Models)\n",
    "from sklearn.linear_model import LogisticRegression  # 로지스틱 회귀\n",
    "\n",
    "# 거리 기반 (Distance-based)\n",
    "from sklearn.neighbors import KNeighborsClassifier   # k-최근접 이웃\n",
    "\n",
    "# 트리 기반 (Tree-based)\n",
    "from sklearn.tree import DecisionTreeClassifier      # 결정나무\n",
    "from sklearn.ensemble import RandomForestClassifier  # 랜덤 포레스트\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # 그래디언트 부스팅\n",
    "\n",
    "# 서포트 벡터 머신 (SVM)\n",
    "from sklearn.svm import SVC                          # 서포트 벡터 분류\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import clone\n",
    "plt.rcParams['font.family'] = 'Apple SD Gothic Neo'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67e6c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "  Train Acc: 1.0\n",
      "  Valid Acc: 0.745\n",
      "  Gap(Train-Valid): 0.255\n",
      "  CV Mean: 0.7312857142857143 (+/- 0.008273389848897288)\n",
      "\n",
      "GradientBoostingClassifier\n",
      "  Train Acc: 0.77375\n",
      "  Valid Acc: 0.7364285714285714\n",
      "  Gap(Train-Valid): 0.037321428571428616\n",
      "  CV Mean: 0.7282857142857143 (+/- 0.005707138387268055)\n",
      "\n",
      "GradientBoostingClassifier\n",
      "  Train Acc: 0.77375\n",
      "  Valid Acc: 0.7364285714285714\n",
      "  Gap(Train-Valid): 0.037321428571428616\n",
      "  CV Mean: 0.7282857142857143 (+/- 0.005707138387268055)\n",
      "\n",
      "XGBClassifier\n",
      "  Train Acc: 0.9864285714285714\n",
      "  Valid Acc: 0.7192857142857143\n",
      "  Gap(Train-Valid): 0.2671428571428571\n",
      "  CV Mean: 0.726 (+/- 0.005795846507171663)\n",
      "\n",
      "SVC\n",
      "  Train Acc: 0.6992857142857143\n",
      "  Valid Acc: 0.7157142857142857\n",
      "  Gap(Train-Valid): -0.01642857142857146\n",
      "  CV Mean: 0.7009999999999998 (+/- 0.014530755559944491)\n",
      "\n",
      "LogisticRegression\n",
      "  Train Acc: 0.7067857142857142\n",
      "  Valid Acc: 0.715\n",
      "  Gap(Train-Valid): -0.00821428571428573\n",
      "  CV Mean: 0.7064285714285714 (+/- 0.00769309258162075)\n",
      "\n",
      "DecisionTreeClassifier\n",
      "  Train Acc: 1.0\n",
      "  Valid Acc: 0.665\n",
      "  Gap(Train-Valid): 0.33499999999999996\n",
      "  CV Mean: 0.6777142857142857 (+/- 0.007946247991527669)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('LogisticRegression',LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('DecisionTreeClassifier',DecisionTreeClassifier(random_state=42)),\n",
    "    ('RandomForestClassifier',RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=42)),\n",
    "    ('SVC',SVC(probability=True, random_state=42)),\n",
    "    ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBClassifier',XGBClassifier(random_state=42)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name,base_model in models:\n",
    "    model = clone(base_model)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    valid_acc = accuracy_score(y_valid, y_valid_pred)\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        base_model, X, y, cv=5, scoring='accuracy'\n",
    "        )\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": name,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"valid_acc\": valid_acc,\n",
    "        \"cv_mean\": cv_mean,\n",
    "        \"cv_std\": cv_std,\n",
    "    })\n",
    "\n",
    "    results_sorted = sorted(results, key=lambda x: x[\"valid_acc\"], reverse=True)\n",
    "\n",
    "for r in results_sorted:\n",
    "    gap = r[\"train_acc\"] - r[\"valid_acc\"]\n",
    "    print(\n",
    "                f\"{r['name']}\\n\"\n",
    "        f\"  Train Acc: {r['train_acc']}\\n\"\n",
    "        f\"  Valid Acc: {r['valid_acc']}\\n\"\n",
    "        f\"  Gap(Train-Valid): {gap}\\n\"\n",
    "        f\"  CV Mean: {r['cv_mean']} (+/- {r['cv_std']})\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092af34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab11ed",
   "metadata": {},
   "source": [
    "# 1차 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c83930",
   "metadata": {},
   "source": [
    "```python\n",
    "RandomForestClassifier\n",
    "  Train Acc: 1.0\n",
    "  Valid Acc: 0.745\n",
    "  Gap(Train-Valid): 0.255\n",
    "  CV Mean: 0.7312857142857143 (+/- 0.008273389848897288)\n",
    "\n",
    "GradientBoostingClassifier\n",
    "  Train Acc: 0.77375\n",
    "  Valid Acc: 0.7364285714285714\n",
    "  Gap(Train-Valid): 0.037321428571428616\n",
    "  CV Mean: 0.7282857142857143 (+/- 0.005707138387268055)\n",
    "\n",
    "GradientBoostingClassifier\n",
    "  Train Acc: 0.77375\n",
    "  Valid Acc: 0.7364285714285714\n",
    "  Gap(Train-Valid): 0.037321428571428616\n",
    "  CV Mean: 0.7282857142857143 (+/- 0.005707138387268055)\n",
    "\n",
    "XGBClassifier\n",
    "  Train Acc: 0.9864285714285714\n",
    "  Valid Acc: 0.7192857142857143\n",
    "  Gap(Train-Valid): 0.2671428571428571\n",
    "  CV Mean: 0.726 (+/- 0.005795846507171663)\n",
    "\n",
    "SVC\n",
    "  Train Acc: 0.6992857142857143\n",
    "  Valid Acc: 0.7157142857142857\n",
    "  Gap(Train-Valid): -0.01642857142857146\n",
    "  CV Mean: 0.7009999999999998 (+/- 0.014530755559944491)\n",
    "\n",
    "LogisticRegression\n",
    "  Train Acc: 0.7067857142857142\n",
    "  Valid Acc: 0.715\n",
    "  Gap(Train-Valid): -0.00821428571428573\n",
    "  CV Mean: 0.7064285714285714 (+/- 0.00769309258162075)\n",
    "\n",
    "DecisionTreeClassifier\n",
    "  Train Acc: 1.0\n",
    "  Valid Acc: 0.665\n",
    "  Gap(Train-Valid): 0.33499999999999996\n",
    "  CV Mean: 0.6777142857142857 (+/- 0.007946247991527669)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345d13d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4866329",
   "metadata": {},
   "source": [
    "# 2차 성능 평가 전처리x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1be7b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['label','ID',], axis = 1)\n",
    "y = train['label']\n",
    "X_test = test.drop(['ID',], axis = 1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b599cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "  Train Acc: 1.0\n",
      "  Valid Acc: 0.735\n",
      "  Gap(Train-Valid): 0.265\n",
      "  CV Mean: 0.7390000000000001 (+/- 0.005952190473142742)\n",
      "\n",
      "LogisticRegression\n",
      "  Train Acc: 0.7046428571428571\n",
      "  Valid Acc: 0.7235714285714285\n",
      "  Gap(Train-Valid): -0.018928571428571406\n",
      "  CV Mean: 0.707 (+/- 0.008752842104023082)\n",
      "\n",
      "GradientBoostingClassifier\n",
      "  Train Acc: 0.7728571428571429\n",
      "  Valid Acc: 0.7235714285714285\n",
      "  Gap(Train-Valid): 0.04928571428571438\n",
      "  CV Mean: 0.7241428571428572 (+/- 0.007047940790596602)\n",
      "\n",
      "GradientBoostingClassifier\n",
      "  Train Acc: 0.7728571428571429\n",
      "  Valid Acc: 0.7235714285714285\n",
      "  Gap(Train-Valid): 0.04928571428571438\n",
      "  CV Mean: 0.7241428571428572 (+/- 0.007047940790596602)\n",
      "\n",
      "XGBClassifier\n",
      "  Train Acc: 0.9825\n",
      "  Valid Acc: 0.7228571428571429\n",
      "  Gap(Train-Valid): 0.2596428571428572\n",
      "  CV Mean: 0.7172857142857143 (+/- 0.003539860483818271)\n",
      "\n",
      "SVC\n",
      "  Train Acc: 0.695\n",
      "  Valid Acc: 0.7121428571428572\n",
      "  Gap(Train-Valid): -0.017142857142857237\n",
      "  CV Mean: 0.6962857142857143 (+/- 0.011878638003421404)\n",
      "\n",
      "DecisionTreeClassifier\n",
      "  Train Acc: 1.0\n",
      "  Valid Acc: 0.6764285714285714\n",
      "  Gap(Train-Valid): 0.3235714285714286\n",
      "  CV Mean: 0.672 (+/- 0.010855262995150225)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('LogisticRegression',LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('DecisionTreeClassifier',DecisionTreeClassifier(random_state=42)),\n",
    "    ('RandomForestClassifier',RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=42)),\n",
    "    ('SVC',SVC(probability=True, random_state=42)),\n",
    "    ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBClassifier',XGBClassifier(random_state=42)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name,base_model in models:\n",
    "    model = clone(base_model)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    valid_acc = accuracy_score(y_valid, y_valid_pred)\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        base_model, X, y, cv=5, scoring='accuracy'\n",
    "        )\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": name,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"valid_acc\": valid_acc,\n",
    "        \"cv_mean\": cv_mean,\n",
    "        \"cv_std\": cv_std,\n",
    "    })\n",
    "\n",
    "    results_sorted = sorted(results, key=lambda x: x[\"valid_acc\"], reverse=True)\n",
    "\n",
    "for r in results_sorted:\n",
    "    gap = r[\"train_acc\"] - r[\"valid_acc\"]\n",
    "    print(\n",
    "                f\"{r['name']}\\n\"\n",
    "        f\"  Train Acc: {r['train_acc']}\\n\"\n",
    "        f\"  Valid Acc: {r['valid_acc']}\\n\"\n",
    "        f\"  Gap(Train-Valid): {gap}\\n\"\n",
    "        f\"  CV Mean: {r['cv_mean']} (+/- {r['cv_std']})\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e9247",
   "metadata": {},
   "source": [
    "```python\n",
    "RandomForestClassifier\n",
    "  Train Acc: 1.0\n",
    "  Valid Acc: 0.735\n",
    "  Gap(Train-Valid): 0.265\n",
    "  CV Mean: 0.7390000000000001 (+/- 0.005952190473142742)\n",
    "\n",
    "LogisticRegression\n",
    "  Train Acc: 0.7046428571428571\n",
    "  Valid Acc: 0.7235714285714285\n",
    "  Gap(Train-Valid): -0.018928571428571406\n",
    "  CV Mean: 0.707 (+/- 0.008752842104023082)\n",
    "\n",
    "GradientBoostingClassifier\n",
    "  Train Acc: 0.7728571428571429\n",
    "  Valid Acc: 0.7235714285714285\n",
    "  Gap(Train-Valid): 0.04928571428571438\n",
    "  CV Mean: 0.7241428571428572 (+/- 0.007047940790596602)\n",
    "\n",
    "GradientBoostingClassifier\n",
    "  Train Acc: 0.7728571428571429\n",
    "  Valid Acc: 0.7235714285714285\n",
    "  Gap(Train-Valid): 0.04928571428571438\n",
    "  CV Mean: 0.7241428571428572 (+/- 0.007047940790596602)\n",
    "\n",
    "XGBClassifier\n",
    "  Train Acc: 0.9825\n",
    "  Valid Acc: 0.7228571428571429\n",
    "  Gap(Train-Valid): 0.2596428571428572\n",
    "  CV Mean: 0.7172857142857143 (+/- 0.003539860483818271)\n",
    "\n",
    "SVC\n",
    "  Train Acc: 0.695\n",
    "  Valid Acc: 0.7121428571428572\n",
    "  Gap(Train-Valid): -0.017142857142857237\n",
    "  CV Mean: 0.6962857142857143 (+/- 0.011878638003421404)\n",
    "\n",
    "DecisionTreeClassifier\n",
    "  Train Acc: 1.0\n",
    "  Valid Acc: 0.6764285714285714\n",
    "  Gap(Train-Valid): 0.3235714285714286\n",
    "  CV Mean: 0.672 (+/- 0.010855262995150225)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f2a61",
   "metadata": {},
   "source": [
    "# 비교 표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938ad83",
   "metadata": {},
   "source": [
    "| 모델                         | 1차 Train Acc(훈련 정확도) | 1차 Valid Acc(검증 정확도) | 1차 Gap(Train-Valid) | 1차 CV Mean | 2차 Train Acc | 2차 Valid Acc | 2차 Gap(Train-Valid) | 2차 CV Mean |\n",
    "| -------------------------- | -------------------- | -------------------- | ------------------- | ---------- | ------------ | ------------ | ------------------- | ---------- |\n",
    "| RandomForestClassifier     | 1.0000               | 0.7450               | 0.2550              | 0.7313     | 1.0000       | 0.7350       | 0.2650              | 0.7390     |\n",
    "| GradientBoostingClassifier | 0.7738               | 0.7364               | 0.0373              | 0.7283     | 0.7729       | 0.7236       | 0.0493              | 0.7241     |\n",
    "| XGBClassifier              | 0.9864               | 0.7193               | 0.2671              | 0.7260     | 0.9825       | 0.7229       | 0.2596              | 0.7173     |\n",
    "| SVC                        | 0.6993               | 0.7157               | -0.0164             | 0.7010     | 0.6950       | 0.7121       | -0.0171             | 0.6963     |\n",
    "| LogisticRegression         | 0.7068               | 0.7150               | -0.0082             | 0.7064     | 0.7046       | 0.7236       | -0.0189             | 0.7070     |\n",
    "| DecisionTreeClassifier     | 1.0000               | 0.6650               | 0.3350              | 0.6777     | 1.0000       | 0.6764       | 0.3236              | 0.6720     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335ab35",
   "metadata": {},
   "source": [
    "| 모델                           | 1차 Valid | 2차 Valid | 차이(1차−2차)   | 해석                 |\n",
    "| ---------------------------- | -------- | -------- | ----------- | ------------------ |\n",
    "| RandomForest (랜덤 포레스트)       | 0.7450   | 0.7350   | **+0.0100** | 파생변수가 약간 도움        |\n",
    "| GradientBoosting (그래디언트 부스팅) | 0.7364   | 0.7236   | **+0.0129** | 파생변수 덕분에 ~1.3%p 상승 |\n",
    "| XGB                          | 0.7193   | 0.7229   | **−0.0036** | 거의 비슷, 2차가 약간 우위   |\n",
    "| SVC                          | 0.7157   | 0.7121   | **+0.0036** | 미세하게 1차가 낫지만 거의 동일 |\n",
    "| LogisticRegression (로지스틱 회귀) | 0.7150   | 0.7236   | **−0.0086** | 원시데이터 쪽이 더 잘 맞음    |\n",
    "| DecisionTree (단일 트리)         | 0.6650   | 0.6764   | **−0.0114** | 원시데이터 쪽이 조금 더 나음   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53e7da",
   "metadata": {},
   "source": [
    "---\n",
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e990a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "probs = models.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "best_acc = 0\n",
    "best_thr = 0.5\n",
    "\n",
    "for thr in np.linspace(0.1, 0.9, 17):  # 0.1 ~ 0.9 사이 0.05 간격\n",
    "    y_pred_thr = (probs >= thr).astype(int)\n",
    "    acc = accuracy_score(y_valid, y_pred_thr)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_thr = thr\n",
    "\n",
    "print(\"Best threshold:\", best_thr)\n",
    "print(\"Best val accuracy:\", best_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
